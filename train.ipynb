{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e545301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.9.1-cp39-cp39-manylinux1_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3 MB 27.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch==1.8.1 in /home/krajda/misinformation/lib/python3.9/site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/krajda/misinformation/lib/python3.9/site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: numpy in /home/krajda/misinformation/lib/python3.9/site-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: typing-extensions in /home/krajda/misinformation/lib/python3.9/site-packages (from torch==1.8.1->torchvision) (3.10.0.0)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.9.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/home/krajda/misinformation/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "56c3ec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.functional import accuracy, f1\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from pytorch_lightning import loggers as pl_loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bbcbf6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    lr = 1e-5\n",
    "    max_len = 512\n",
    "    train_bs = 64\n",
    "    valid_bs = 64\n",
    "    train_pcent = 0.80\n",
    "    num_workers = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8868a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset( ):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.tokenizer = transformers.XLMTokenizer.from_pretrained(\"allegro/herbert-klej-cased-tokenizer-v1\")\n",
    "\n",
    "        self.inputs = self.tokenize(texts)\n",
    "        self.labels = labels                       \n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def tokenize(self, texts):\n",
    "        return self.tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            None,\n",
    "            pad_to_max_length=True,\n",
    "            max_length = 512,\n",
    "            truncation='longest_first',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return (self.inputs['input_ids'][idx],\n",
    "        self.inputs['attention_mask'][idx],\n",
    "        self.inputs['token_type_ids'][idx],\n",
    "        self.labels[idx])\n",
    "\n",
    "\n",
    "class SentimentDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        df = pd.read_csv('./advertising_sentiment_dataset.csv')\n",
    "        df = df[['text', 'sentiment']]\n",
    "\n",
    "        sentimap = {'Positive': 1, 'Negative': 2, 'Neutral': 0}\n",
    "\n",
    "        df['sentiment'] = df.apply(lambda x: sentimap[x['sentiment']] if x['sentiment'] is not np.nan else 0, axis=1)\n",
    "\n",
    "        self.data = df\n",
    "\n",
    "    def setup(self, stage):\n",
    "        nb_training_samples = (int)(len(self.data)*0.8)\n",
    "        \n",
    "        \n",
    "        self.training_set = SentimentDataset(\n",
    "            texts=self.data[:nb_training_samples]['text'].values,\n",
    "            labels=self.data[:nb_training_samples]['sentiment'].values,\n",
    "        )\n",
    "\n",
    "        self.validation_set = SentimentDataset(\n",
    "            texts=self.data[nb_training_samples:]['text'].values,\n",
    "            labels=self.data[nb_training_samples:]['sentiment'].values,\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.training_set, batch_size=64)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.validation_set, batch_size=64)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97905812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "#         self.save_hyperparameters()\n",
    "        \n",
    "        self.pretrained_model = transformers.RobertaModel.from_pretrained(\"laugustyniak/roberta-polish-web-embedding-v1\")\n",
    "        \n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.model = nn.Sequential(nn.Dropout(0.5),\n",
    "                                nn.Linear(self.pretrained_model.config.hidden_size, 256), nn.ReLU(),\n",
    "                                nn.Linear(256, 16), nn.ReLU(),\n",
    "                                nn.Linear(16, 3), nn.ReLU())\n",
    "        self.loss = F.cross_entropy\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids) -> torch.Tensor:\n",
    "        embeddings = self.pretrained_model(input_ids=input_ids,\n",
    "                         attention_mask=attention_mask,\n",
    "                         token_type_ids=token_type_ids,\n",
    "                         output_hidden_states=True)\n",
    "\n",
    "        logits = self.model(embeddings['pooler_output'])\n",
    "        return logits\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        \n",
    "        input_ids, attention_mask, token_type_ids, label = batch\n",
    "        \n",
    "        y = self(input_ids, attention_mask, token_type_ids)\n",
    "        loss = self.loss(y, label)\n",
    "        \n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def log_metrics(self, y, label, step):\n",
    "        y_soft = self.softmax(y)\n",
    "        loss = self.loss(y, label)\n",
    "        acc = accuracy(y_soft, label)\n",
    "        f_1 = f1(y_soft, label)\n",
    "        \n",
    "        self.log(f\"{step}_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(f\"{step}_acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(f\"{step}_f1\", f_1,on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        input_ids, attention_mask, token_type_ids, label = batch\n",
    "        y = self(input_ids, attention_mask, token_type_ids)\n",
    "        self.log_metrics(y, label, 'val')       \n",
    "\n",
    "        \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        input_ids, attention_mask, token_type_ids, label = batch\n",
    "        y = self(input_ids, attention_mask, token_type_ids)\n",
    "        self.log_metrics(y, label, 'test')  \n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=Config.lr)\n",
    "    \n",
    "    def predict(self, texts: list) -> dict:\n",
    "        with torch.no_grad():\n",
    "            model_input = SentimentDataset(texts, None).inputs\n",
    "            model_out = self(**model_input)\n",
    "            model_out = self.softmax(model_out)\n",
    "            model_out = model_out.numpy()\n",
    "            return np.argmax(model_out, axis=1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7be05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4]\n",
      "\n",
      "  | Name             | Type         | Params\n",
      "--------------------------------------------------\n",
      "0 | pretrained_model | RobertaModel | 124 M \n",
      "1 | model            | Sequential   | 201 K \n",
      "2 | softmax          | Softmax      | 0     \n",
      "--------------------------------------------------\n",
      "201 K     Trainable params\n",
      "124 M     Non-trainable params\n",
      "125 M     Total params\n",
      "500.293   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed4dccedef648f4a9b21cd15301afd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03a4b4a56654a95ae33226693f805cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model()\n",
    "datamodule = SentimentDataModule()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=pl_loggers.TensorBoardLogger('logs/'),\n",
    "    max_epochs=100, \n",
    "    gpus=1,\n",
    ")\n",
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc934cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = ['ala ma kota', 'duda to debil', 'O Narodowej Strategii Onkologicznej : \\ \" Pan prezydent hucznie ją ogłosił , ale to nie jest strategia . Taki dokument powinien zawierać cele i działania służące ich realizacji , harmonogram , kosztorys , mierniki i sposoby oceny . Nie zawiera . Traktuję to jako element kampanii wyborczej \\ \" https://twitter.com/mamago25/status/1232230675870253056']\n",
    "\n",
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d47298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bccf20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
